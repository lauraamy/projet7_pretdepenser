{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ad8f5b-20eb-49db-a9a6-91a8bc731d37",
   "metadata": {},
   "source": [
    "# Exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d1a7a9-654f-494e-9af1-45e1d99f84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02329a-86df-4709-9cb6-1d6a06b1760a",
   "metadata": {},
   "source": [
    "### Read in all the Home Credit Default Risk csv files\n",
    "- Merge them into one\n",
    "- Create dummy variables where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e54947-5353-417a-a651-4e74e3a54c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = timeit.timeit()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, timeit.timeit() - t0))\n",
    "    \n",
    "#start_time = timeit.default_timer()   \n",
    "#elapsed = timeit.default_timer() - start_time\n",
    "#executionTimeMin = elapsed / 60\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    #check which cols are categorical \n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    #get dummy variables for categorical variables\n",
    "    df = pd.get_dummies(df, columns = categorical_columns, dummy_na = nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    \n",
    "    # put the two datasets together\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        # Encode the object as an enumerated type or categorical variable.\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/bureau_balance.csv', nrows = num_rows)\n",
    "\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    # bb is the df and bb_cat are the new cols\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    # in the new cols of the bureau balance\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "        #print(bb_aggregations)\n",
    "        \n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    \n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "\n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/installments_payments.csv', \n",
    "                      nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f551ba-5556-49ee-9df9-5ab7f924c042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 116)\n",
      "Process bureau and bureau_balance - done in 0s\n",
      "Previous applications df shape: (338857, 249)\n",
      "Process previous_applications - done in -0s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in -0s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in -0s\n"
     ]
    }
   ],
   "source": [
    "# launch all functions to obtain a final merged df\n",
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df\n",
    "    #with timer(\"Run LightGBM with kfold\"):\n",
    "    #    feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #submission_file_name = \"../my_csv_files/submission_kernel02.csv\"\n",
    "    #with timer(\"Full model run\"):\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a5818c6-ac46-4000-b2e4-49285fe598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the merged df to a csv file\n",
    "df.to_csv('../my_csv_files/MY_merged_all_files.csv')\n",
    "df_merged_all = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6eca4-809c-487f-8d1d-aa9cece140b8",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0efb5c2-664c-4dce-aa6a-702a4a900add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        \"\"\"\n",
    "        df : data for which you want to know the missing values\n",
    "        \n",
    "        Returns: a table with the missing value for each feature and the percentage of missing values\n",
    "        \"\"\"\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns.sort_values('% of Total Values', \n",
    "                                                                          ascending=False).round(1)\n",
    "        #mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        #    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        #'% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "               \"There are \" + str(mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].shape[0]) +\n",
    "               \" columns that have missing values.\")\n",
    "        \n",
    "        #print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "        #    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "        #      \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da643a36-d000-4e70-8c10-7a1c4b0cbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 798 columns.\n",
      "There are 611 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REFUSED_RATE_DOWN_PAYMENT_MAX</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_RATE_DOWN_PAYMENT_MIN</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_AMT_DOWN_PAYMENT_MEAN</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_RATE_DOWN_PAYMENT_MEAN</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_AMT_DOWN_PAYMENT_MAX</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Government</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Emergency</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Electricity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Culture</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Missing Values  % of Total Values\n",
       "REFUSED_RATE_DOWN_PAYMENT_MAX           303648               85.2\n",
       "REFUSED_RATE_DOWN_PAYMENT_MIN           303648               85.2\n",
       "REFUSED_AMT_DOWN_PAYMENT_MEAN           303648               85.2\n",
       "REFUSED_RATE_DOWN_PAYMENT_MEAN          303648               85.2\n",
       "REFUSED_AMT_DOWN_PAYMENT_MAX            303648               85.2\n",
       "...                                        ...                ...\n",
       "ORGANIZATION_TYPE_Government                 0                0.0\n",
       "ORGANIZATION_TYPE_Emergency                  0                0.0\n",
       "ORGANIZATION_TYPE_Electricity                0                0.0\n",
       "ORGANIZATION_TYPE_Culture                    0                0.0\n",
       "index                                        0                0.0\n",
       "\n",
       "[798 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values of the merged df \n",
    "missing_values_table(df_merged_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31b912c-eb7f-444c-8586-234af8dc6001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 798)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fecab-314c-430f-8cfd-05c82577bbfe",
   "metadata": {},
   "source": [
    "# Exploration with Pycaret\n",
    "(see p7_notebook_pycaret.ipynb)\n",
    "Comparison of models and use of SMOTE (oversampling) and undersampling on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d0c87-c611-451d-a7d2-f23ddbc48a2b",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd9571-3263-42e5-b35e-1a259bafc4c6",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c78da84-c492-4853-a665-e9f3e32dd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the merged file, and keep only the rows where the TARGET value is not null\n",
    "df_notnull_target = df_merged_all[df_merged_all['TARGET'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7767c5bf-44ca-42aa-b318-54c831f9f75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notnull_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741585d2-12b7-439a-a90b-0243820d9d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307488, 798)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many rows would be lost if we were to remove all rows containing inf\n",
    "\n",
    "#df_merged_all[~df_merged_all.isin([np.inf, -np.inf, np.nan]).any(1)].shape\n",
    "df_notnull_target[~df_notnull_target.isin([np.inf, -np.inf]).any(1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f51d3ee-55c2-42e8-aa8c-4292135e649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 798 columns.\n",
      "There are 610 columns that have missing values.\n"
     ]
    }
   ],
   "source": [
    "# remove any inf or -inf values in the dataset, and then get the missing values \n",
    "missing_values = missing_values_table(df_notnull_target[~df_notnull_target.isin([np.inf, -np.inf]).any(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec910dd5-c050-4b90-bd0f-b234859d0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the missing values df to a csv file \n",
    "missing_values.to_csv(\"../my_csv_files/missing_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebd46a-f9b1-4a10-89fa-a40c698f0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.tail(546)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f53fd2-8a2c-42f0-af98-65973e5e47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that have the least missing values, for feature selection combining least missing values \n",
    "# and most important features\n",
    "data_for_feat_selec1 = missing_values.tail(545)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85f1eb-da34-44cb-9dd8-ad0f4c3b5344",
   "metadata": {},
   "source": [
    "### Preparing the data for : lightgbm for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c472fb-2268-4960-872a-276621be7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2201f399-b76a-475b-b238-f2934cc90d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns if needed\n",
    "df_notnull_target = df_notnull_target.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Create a list of features that will be used for modelling \n",
    "feats = [f for f in df_notnull_target.columns if f not in ['TARGET', 'Unnamed: 0', 'Unnamed0',\n",
    "                                                  'SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89a48a-2099-41c2-960e-63d0894cdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide the data into training and test data \n",
    "x_train, x_test, y_train, y_test = train_test_split(df_notnull_targetl[feats], \n",
    "                                                    df_notnull_target['TARGET'], \n",
    "                                                    test_size = 0.2, random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfb54216-490b-4835-8a87-db3599f290e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_comp_score_lgbm(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: true Target values\n",
    "    y_pred : our predicted Target values\n",
    "    \n",
    "    Returns: our custom score based on our custom evaluation metric\n",
    "    \"\"\"\n",
    "    # for now y_pred is the probability that the value is 1\n",
    "    y_use = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "    \n",
    "    y_use = pd.Series(y_use)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_use).ravel()\n",
    "    \n",
    "    tn_weight = 1\n",
    "    fp_weight = 0\n",
    "    fn_weight = -10\n",
    "    tp_weight = 0\n",
    "\n",
    "    # gain function for company, true positives and false positives don't matter that much to us\n",
    "    # we want to penalise the false negatives, and we want to say yes to true negatives\n",
    "    gain = tp*(tp_weight) + tn*(tn_weight) + fp*(fp_weight) + fn*(fn_weight)\n",
    "    \n",
    "    # best represents scenario where there are no false negatives or false postives\n",
    "    # so all false positives would be correctly shown as negative\n",
    "    # and all false negatives would be correctly show as positive\n",
    "    best = (tn + fp) * tn_weight + (tp + fn) * tp_weight\n",
    "    \n",
    "    # baseline is a naive model that predicts non default(negative) for everyone\n",
    "    # but all true positives and false negatives would be incorrectly shown as negative\n",
    "    baseline = (tn + fp) * tn_weight + (tp + fn) * fn_weight\n",
    "    \n",
    "    score = ((gain - baseline) / (best - baseline))\n",
    "\n",
    "    return 'my score', score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20dec749-826c-4fcd-ac81-1ae02626ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for LGBM modelling\n",
    "\n",
    "num_folds= 10\n",
    "stratified= False\n",
    "\n",
    "# Cross validation model\n",
    "if stratified:\n",
    "    folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "else:\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    \n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(df_notnull_target.shape[0])\n",
    "#sub_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8585be5-70ee-48c2-9c72-5639532be67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop, going through each fold and using the LGBM hyperparameters that were in the Kaggle\n",
    "# kernel the exploration and cleaning code was inspired from\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train[feats], \n",
    "                                                            y_train)):\n",
    "    train_x, train_y = df_notnull_target[feats].iloc[train_idx], df_notnull_target['TARGET'].iloc[train_idx]\n",
    "    valid_x, valid_y = df_notnull_target[feats].iloc[valid_idx], df_notnull_target['TARGET'].iloc[valid_idx]\n",
    "    \n",
    "    valid_x.to_csv(\"../my_csv_files/MY_valid_x.csv\")\n",
    "    valid_y.to_csv(\"../my_csv_files/MY_valid_y.csv\")\n",
    "    \n",
    "    # Light GBM parameters found by my hyperparameter search\n",
    "    clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "    \n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            verbose= 200, early_stopping_rounds= 200, eval_metric = my_comp_score_lgbm)\n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    \n",
    "    valid_preds = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n",
    "    valid_preds_proba = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    # create a df for feature importance\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis = 0)\n",
    "    \n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "    del train_x, train_y\n",
    "    #valid_x, valid_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92c434-ba6a-4e85-b2b7-5a0c198bacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the important features for this model to a csv file to read in the next notebook\n",
    "feature_importance_df[[\"feature\", \"importance\"]].\\\n",
    "                        groupby(\"feature\").mean().\\\n",
    "                        sort_values(by = \"importance\", \n",
    "                                    ascending=False)[:115].\\\n",
    "                                    to_csv('../my_csv_files/feats_lgbm_mergedcsv_good.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c428e4-18a9-477d-b815-ba9a87f164fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
