{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ad8f5b-20eb-49db-a9a6-91a8bc731d37",
   "metadata": {},
   "source": [
    "# Exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d1a7a9-654f-494e-9af1-45e1d99f84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02329a-86df-4709-9cb6-1d6a06b1760a",
   "metadata": {},
   "source": [
    "### Read in all the Credit Home csv files \n",
    "- Merge them into one\n",
    "- Create Dummy Variables where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e54947-5353-417a-a651-4e74e3a54c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = timeit.timeit()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, timeit.timeit() - t0))\n",
    "    \n",
    "#start_time = timeit.default_timer()   \n",
    "#elapsed = timeit.default_timer() - start_time\n",
    "#executionTimeMin = elapsed / 60\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    #check which cols are categorical \n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    #get dummy variables for categorical variables\n",
    "    df = pd.get_dummies(df, columns = categorical_columns, dummy_na = nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    \n",
    "    # put the two datasets together\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        # Encode the object as an enumerated type or categorical variable.\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/bureau_balance.csv', nrows = num_rows)\n",
    "\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    # bb is the df and bb_cat are the new cols\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    # in the new cols of the bureau balance\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "        #print(bb_aggregations)\n",
    "        \n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    \n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "\n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/installments_payments.csv', \n",
    "                      nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('../Projet+Mise+en+prod+-+home-credit-default-risk/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f551ba-5556-49ee-9df9-5ab7f924c042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 116)\n",
      "Process bureau and bureau_balance - done in 0s\n",
      "Previous applications df shape: (338857, 249)\n",
      "Process previous_applications - done in -0s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in -0s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in -0s\n"
     ]
    }
   ],
   "source": [
    "# launch all functions to obtain merged csv file\n",
    "\n",
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df\n",
    "    #with timer(\"Run LightGBM with kfold\"):\n",
    "    #    feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #submission_file_name = \"../my_csv_files/submission_kernel02.csv\"\n",
    "    #with timer(\"Full model run\"):\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a5818c6-ac46-4000-b2e4-49285fe598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../my_csv_files/MY_merged_all_files.csv')\n",
    "df_merged_all = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6eca4-809c-487f-8d1d-aa9cece140b8",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0efb5c2-664c-4dce-aa6a-702a4a900add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        \"\"\"\n",
    "        df : data for which you want to know the missing values\n",
    "        \"\"\"\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns.sort_values('% of Total Values', \n",
    "                                                                          ascending=False).round(1)\n",
    "        #mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        #    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        #'% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "               \"There are \" + str(mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].shape[0]) +\n",
    "               \" columns that have missing values.\")\n",
    "        \n",
    "        #print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "        #    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "        #      \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da643a36-d000-4e70-8c10-7a1c4b0cbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 798 columns.\n",
      "There are 611 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REFUSED_RATE_DOWN_PAYMENT_MAX</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_RATE_DOWN_PAYMENT_MIN</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_AMT_DOWN_PAYMENT_MEAN</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_RATE_DOWN_PAYMENT_MEAN</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_AMT_DOWN_PAYMENT_MAX</th>\n",
       "      <td>303648</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Government</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Emergency</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Electricity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Culture</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Missing Values  % of Total Values\n",
       "REFUSED_RATE_DOWN_PAYMENT_MAX           303648               85.2\n",
       "REFUSED_RATE_DOWN_PAYMENT_MIN           303648               85.2\n",
       "REFUSED_AMT_DOWN_PAYMENT_MEAN           303648               85.2\n",
       "REFUSED_RATE_DOWN_PAYMENT_MEAN          303648               85.2\n",
       "REFUSED_AMT_DOWN_PAYMENT_MAX            303648               85.2\n",
       "...                                        ...                ...\n",
       "ORGANIZATION_TYPE_Government                 0                0.0\n",
       "ORGANIZATION_TYPE_Emergency                  0                0.0\n",
       "ORGANIZATION_TYPE_Electricity                0                0.0\n",
       "ORGANIZATION_TYPE_Culture                    0                0.0\n",
       "index                                        0                0.0\n",
       "\n",
       "[798 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(df_merged_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31b912c-eb7f-444c-8586-234af8dc6001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 798)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fecab-314c-430f-8cfd-05c82577bbfe",
   "metadata": {},
   "source": [
    "# Exploration with Pycaret\n",
    "(see p7_notebook_pycaret.ipynb)\n",
    "Explored using SMOTE on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d0c87-c611-451d-a7d2-f23ddbc48a2b",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd9571-3263-42e5-b35e-1a259bafc4c6",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c78da84-c492-4853-a665-e9f3e32dd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notnull_target = df_merged_all[df_merged_all['TARGET'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7767c5bf-44ca-42aa-b318-54c831f9f75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notnull_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741585d2-12b7-439a-a90b-0243820d9d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307488, 798)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_merged_all[~df_merged_all.isin([np.inf, -np.inf, np.nan]).any(1)].shape\n",
    "df_notnull_target[~df_notnull_target.isin([np.inf, -np.inf]).any(1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f51d3ee-55c2-42e8-aa8c-4292135e649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 798 columns.\n",
      "There are 610 columns that have missing values.\n"
     ]
    }
   ],
   "source": [
    "# remove any inf or -inf values in the dataset\n",
    "missing_values = missing_values_table(df_notnull_target[~df_notnull_target.isin([np.inf, -np.inf]).any(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec910dd5-c050-4b90-bd0f-b234859d0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.to_csv(\"../my_csv_files/missing_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebd46a-f9b1-4a10-89fa-a40c698f0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.tail(546)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f53fd2-8a2c-42f0-af98-65973e5e47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that have the least missing values, for feature selection combining least missing values \n",
    "# and most important features\n",
    "data_for_feat_selec1 = missing_values.tail(545)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85f1eb-da34-44cb-9dd8-ad0f4c3b5344",
   "metadata": {},
   "source": [
    "### Preparing the data for : lightgbm for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c472fb-2268-4960-872a-276621be7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2201f399-b76a-475b-b238-f2934cc90d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training/validation and test data\n",
    "\n",
    "df_notnull_target = df_notnull_target.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "feats = [f for f in df_notnull_target.columns if f not in ['TARGET', 'Unnamed: 0', 'Unnamed0',\n",
    "                                                  'SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfb54216-490b-4835-8a87-db3599f290e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_comp_score_lgbm(y_true, y_pred):\n",
    "    \n",
    "    # for now y_pred is the probability that the value is 1\n",
    "    y_use = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "    \n",
    "    y_use = pd.Series(y_use)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_use).ravel()\n",
    "    \n",
    "    tn_weight = 1\n",
    "    fp_weight = 0\n",
    "    fn_weight = -10\n",
    "    tp_weight = 0\n",
    "\n",
    "    # gain function for company, true positives and false positives don't matter that much to us\n",
    "    # we want to penalise the false negatives, and we want to say yes to true negatives\n",
    "    gain = tp*(tp_weight) + tn*(tn_weight) + fp*(fp_weight) + fn*(fn_weight)\n",
    "    \n",
    "    # best represents scenario where there are no false negatives or false postives\n",
    "    # so all false positives would be correctly shown as negative\n",
    "    # and all false negatives would be correctly show as positive\n",
    "    best = (tn + fp) * tn_weight + (tp + fn) * tp_weight\n",
    "    \n",
    "    # baseline is a naive model that predicts non default(negative) for everyone\n",
    "    # but all true positives and false negatives would be incorrectly shown as negative\n",
    "    baseline = (tn + fp) * tn_weight + (tp + fn) * fn_weight\n",
    "    \n",
    "    score = ((gain - baseline) / (best - baseline))\n",
    "\n",
    "    return 'my score', score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20dec749-826c-4fcd-ac81-1ae02626ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds= 10\n",
    "stratified= False\n",
    "\n",
    "# Cross validation model\n",
    "if stratified:\n",
    "    folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "else:\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    \n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(df_notnull_target.shape[0])\n",
    "#sub_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8585be5-70ee-48c2-9c72-5639532be67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.234829\ttraining's my score: 0.0124815\tvalid_1's binary_logloss: 0.246759\tvalid_1's my score: 0.0065625\n",
      "[400]\ttraining's binary_logloss: 0.22537\ttraining's my score: 0.0452459\tvalid_1's binary_logloss: 0.242511\tvalid_1's my score: 0.0291016\n",
      "[600]\ttraining's binary_logloss: 0.219277\ttraining's my score: 0.0614597\tvalid_1's binary_logloss: 0.241208\tvalid_1's my score: 0.0365234\n",
      "[800]\ttraining's binary_logloss: 0.214311\ttraining's my score: 0.0734965\tvalid_1's binary_logloss: 0.240572\tvalid_1's my score: 0.043125\n",
      "[1000]\ttraining's binary_logloss: 0.20991\ttraining's my score: 0.0808219\tvalid_1's binary_logloss: 0.240345\tvalid_1's my score: 0.0453516\n",
      "[1200]\ttraining's binary_logloss: 0.205763\ttraining's my score: 0.088062\tvalid_1's binary_logloss: 0.240111\tvalid_1's my score: 0.0487109\n",
      "[1400]\ttraining's binary_logloss: 0.2017\ttraining's my score: 0.0972243\tvalid_1's binary_logloss: 0.240002\tvalid_1's my score: 0.0521094\n",
      "[1600]\ttraining's binary_logloss: 0.197882\ttraining's my score: 0.106733\tvalid_1's binary_logloss: 0.239989\tvalid_1's my score: 0.0523047\n",
      "Early stopping, best iteration is:\n",
      "[1438]\ttraining's binary_logloss: 0.200982\ttraining's my score: 0.098684\tvalid_1's binary_logloss: 0.239963\tvalid_1's my score: 0.0516797\n",
      "Fold  1 AUC : 0.794511\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235573\ttraining's my score: 0.0131764\tvalid_1's binary_logloss: 0.241702\tvalid_1's my score: 0.00701048\n",
      "[400]\ttraining's binary_logloss: 0.226043\ttraining's my score: 0.0454415\tvalid_1's binary_logloss: 0.237454\tvalid_1's my score: 0.0261483\n",
      "[600]\ttraining's binary_logloss: 0.21997\ttraining's my score: 0.0614779\tvalid_1's binary_logloss: 0.236447\tvalid_1's my score: 0.0371475\n",
      "[800]\ttraining's binary_logloss: 0.214955\ttraining's my score: 0.0723045\tvalid_1's binary_logloss: 0.235906\tvalid_1's my score: 0.0399275\n",
      "[1000]\ttraining's binary_logloss: 0.210675\ttraining's my score: 0.0812872\tvalid_1's binary_logloss: 0.235667\tvalid_1's my score: 0.0440774\n",
      "[1200]\ttraining's binary_logloss: 0.206638\ttraining's my score: 0.0887079\tvalid_1's binary_logloss: 0.23555\tvalid_1's my score: 0.0458098\n",
      "[1400]\ttraining's binary_logloss: 0.202693\ttraining's my score: 0.0974399\tvalid_1's binary_logloss: 0.235424\tvalid_1's my score: 0.0479855\n",
      "[1600]\ttraining's binary_logloss: 0.198861\ttraining's my score: 0.105138\tvalid_1's binary_logloss: 0.23536\tvalid_1's my score: 0.0486301\n",
      "[1800]\ttraining's binary_logloss: 0.195196\ttraining's my score: 0.113015\tvalid_1's binary_logloss: 0.235375\tvalid_1's my score: 0.0492345\n",
      "Early stopping, best iteration is:\n",
      "[1749]\ttraining's binary_logloss: 0.196062\ttraining's my score: 0.111596\tvalid_1's binary_logloss: 0.235343\tvalid_1's my score: 0.0487913\n",
      "Fold  2 AUC : 0.791475\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235026\ttraining's my score: 0.0128228\tvalid_1's binary_logloss: 0.246536\tvalid_1's my score: 0.0113447\n",
      "[400]\ttraining's binary_logloss: 0.225433\ttraining's my score: 0.044844\tvalid_1's binary_logloss: 0.242546\tvalid_1's my score: 0.0270131\n",
      "[600]\ttraining's binary_logloss: 0.219239\ttraining's my score: 0.0613343\tvalid_1's binary_logloss: 0.241422\tvalid_1's my score: 0.0365331\n",
      "[800]\ttraining's binary_logloss: 0.214093\ttraining's my score: 0.0714401\tvalid_1's binary_logloss: 0.240809\tvalid_1's my score: 0.0395081\n",
      "[1000]\ttraining's binary_logloss: 0.209775\ttraining's my score: 0.0810662\tvalid_1's binary_logloss: 0.240494\tvalid_1's my score: 0.0431575\n",
      "[1200]\ttraining's binary_logloss: 0.205683\ttraining's my score: 0.0893024\tvalid_1's binary_logloss: 0.240377\tvalid_1's my score: 0.0460135\n",
      "[1400]\ttraining's binary_logloss: 0.201832\ttraining's my score: 0.0981124\tvalid_1's binary_logloss: 0.240407\tvalid_1's my score: 0.0461325\n",
      "Early stopping, best iteration is:\n",
      "[1363]\ttraining's binary_logloss: 0.202503\ttraining's my score: 0.096319\tvalid_1's binary_logloss: 0.240361\tvalid_1's my score: 0.0459738\n",
      "Fold  3 AUC : 0.785324\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.23554\ttraining's my score: 0.0127461\tvalid_1's binary_logloss: 0.241263\tvalid_1's my score: 0.00897695\n",
      "[400]\ttraining's binary_logloss: 0.225978\ttraining's my score: 0.0447521\tvalid_1's binary_logloss: 0.236955\tvalid_1's my score: 0.0315406\n",
      "[600]\ttraining's binary_logloss: 0.219837\ttraining's my score: 0.0605718\tvalid_1's binary_logloss: 0.235482\tvalid_1's my score: 0.0391832\n",
      "[800]\ttraining's binary_logloss: 0.214924\ttraining's my score: 0.0712554\tvalid_1's binary_logloss: 0.23491\tvalid_1's my score: 0.0427012\n",
      "[1000]\ttraining's binary_logloss: 0.210442\ttraining's my score: 0.0805923\tvalid_1's binary_logloss: 0.234455\tvalid_1's my score: 0.0435908\n",
      "[1200]\ttraining's binary_logloss: 0.206311\ttraining's my score: 0.0893253\tvalid_1's binary_logloss: 0.234325\tvalid_1's my score: 0.0445208\n",
      "[1400]\ttraining's binary_logloss: 0.202334\ttraining's my score: 0.0978928\tvalid_1's binary_logloss: 0.234145\tvalid_1's my score: 0.0454913\n",
      "Early stopping, best iteration is:\n",
      "[1269]\ttraining's binary_logloss: 0.204953\ttraining's my score: 0.0923049\tvalid_1's binary_logloss: 0.234214\tvalid_1's my score: 0.0477153\n",
      "Fold  4 AUC : 0.795498\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.236317\ttraining's my score: 0.0119075\tvalid_1's binary_logloss: 0.234124\tvalid_1's my score: 0.0132941\n",
      "[400]\ttraining's binary_logloss: 0.226741\ttraining's my score: 0.0421107\tvalid_1's binary_logloss: 0.229854\tvalid_1's my score: 0.0376104\n",
      "[600]\ttraining's binary_logloss: 0.220619\ttraining's my score: 0.0595331\tvalid_1's binary_logloss: 0.228569\tvalid_1's my score: 0.0451409\n",
      "[800]\ttraining's binary_logloss: 0.215603\ttraining's my score: 0.0708749\tvalid_1's binary_logloss: 0.227936\tvalid_1's my score: 0.0488851\n",
      "[1000]\ttraining's binary_logloss: 0.211213\ttraining's my score: 0.0801898\tvalid_1's binary_logloss: 0.22756\tvalid_1's my score: 0.0530921\n",
      "[1200]\ttraining's binary_logloss: 0.207091\ttraining's my score: 0.0889567\tvalid_1's binary_logloss: 0.227334\tvalid_1's my score: 0.053597\n",
      "[1400]\ttraining's binary_logloss: 0.203282\ttraining's my score: 0.0963427\tvalid_1's binary_logloss: 0.227263\tvalid_1's my score: 0.0543122\n",
      "Early stopping, best iteration is:\n",
      "[1298]\ttraining's binary_logloss: 0.205234\ttraining's my score: 0.0921953\tvalid_1's binary_logloss: 0.22728\tvalid_1's my score: 0.0548591\n",
      "Fold  5 AUC : 0.797200\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235337\ttraining's my score: 0.0131374\tvalid_1's binary_logloss: 0.243343\tvalid_1's my score: 0.0100041\n",
      "[400]\ttraining's binary_logloss: 0.225758\ttraining's my score: 0.0439882\tvalid_1's binary_logloss: 0.239452\tvalid_1's my score: 0.0297286\n",
      "[600]\ttraining's binary_logloss: 0.219617\ttraining's my score: 0.0619744\tvalid_1's binary_logloss: 0.238116\tvalid_1's my score: 0.036533\n",
      "[800]\ttraining's binary_logloss: 0.214639\ttraining's my score: 0.0729782\tvalid_1's binary_logloss: 0.237496\tvalid_1's my score: 0.0408262\n",
      "[1000]\ttraining's binary_logloss: 0.210211\ttraining's my score: 0.0805824\tvalid_1's binary_logloss: 0.237139\tvalid_1's my score: 0.045808\n",
      "[1200]\ttraining's binary_logloss: 0.205959\ttraining's my score: 0.0889157\tvalid_1's binary_logloss: 0.236995\tvalid_1's my score: 0.0464561\n",
      "[1400]\ttraining's binary_logloss: 0.201915\ttraining's my score: 0.0978127\tvalid_1's binary_logloss: 0.236983\tvalid_1's my score: 0.0491292\n",
      "Early stopping, best iteration is:\n",
      "[1262]\ttraining's binary_logloss: 0.204677\ttraining's my score: 0.0914028\tvalid_1's binary_logloss: 0.236921\tvalid_1's my score: 0.0488052\n",
      "Fold  6 AUC : 0.785344\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235527\ttraining's my score: 0.0122965\tvalid_1's binary_logloss: 0.240657\tvalid_1's my score: 0.00808614\n",
      "[400]\ttraining's binary_logloss: 0.225979\ttraining's my score: 0.0439143\tvalid_1's binary_logloss: 0.236476\tvalid_1's my score: 0.0301503\n",
      "[600]\ttraining's binary_logloss: 0.219851\ttraining's my score: 0.0613978\tvalid_1's binary_logloss: 0.235247\tvalid_1's my score: 0.0382771\n",
      "[800]\ttraining's binary_logloss: 0.21483\ttraining's my score: 0.0727061\tvalid_1's binary_logloss: 0.234726\tvalid_1's my score: 0.0403495\n",
      "[1000]\ttraining's binary_logloss: 0.210487\ttraining's my score: 0.0808889\tvalid_1's binary_logloss: 0.234503\tvalid_1's my score: 0.0451849\n",
      "[1200]\ttraining's binary_logloss: 0.206324\ttraining's my score: 0.0881998\tvalid_1's binary_logloss: 0.234408\tvalid_1's my score: 0.0475416\n",
      "Early stopping, best iteration is:\n",
      "[1181]\ttraining's binary_logloss: 0.206688\ttraining's my score: 0.0874978\tvalid_1's binary_logloss: 0.234428\tvalid_1's my score: 0.0488013\n",
      "Fold  7 AUC : 0.790321\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235077\ttraining's my score: 0.0127381\tvalid_1's binary_logloss: 0.245343\tvalid_1's my score: 0.00930788\n",
      "[400]\ttraining's binary_logloss: 0.225542\ttraining's my score: 0.0445789\tvalid_1's binary_logloss: 0.240982\tvalid_1's my score: 0.0279236\n",
      "[600]\ttraining's binary_logloss: 0.219451\ttraining's my score: 0.0608982\tvalid_1's binary_logloss: 0.239525\tvalid_1's my score: 0.0398568\n",
      "[800]\ttraining's binary_logloss: 0.214629\ttraining's my score: 0.0724082\tvalid_1's binary_logloss: 0.238904\tvalid_1's my score: 0.0447096\n",
      "[1000]\ttraining's binary_logloss: 0.210207\ttraining's my score: 0.0812469\tvalid_1's binary_logloss: 0.238634\tvalid_1's my score: 0.0478123\n",
      "[1200]\ttraining's binary_logloss: 0.206035\ttraining's my score: 0.090247\tvalid_1's binary_logloss: 0.238496\tvalid_1's my score: 0.0488862\n",
      "[1400]\ttraining's binary_logloss: 0.202086\ttraining's my score: 0.0994353\tvalid_1's binary_logloss: 0.238361\tvalid_1's my score: 0.0523071\n",
      "[1600]\ttraining's binary_logloss: 0.19833\ttraining's my score: 0.107629\tvalid_1's binary_logloss: 0.238425\tvalid_1's my score: 0.0531822\n",
      "Early stopping, best iteration is:\n",
      "[1417]\ttraining's binary_logloss: 0.20174\ttraining's my score: 0.099758\tvalid_1's binary_logloss: 0.238351\tvalid_1's my score: 0.0534208\n",
      "Fold  8 AUC : 0.789806\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235491\ttraining's my score: 0.0119857\tvalid_1's binary_logloss: 0.242519\tvalid_1's my score: 0.00922465\n",
      "[400]\ttraining's binary_logloss: 0.225829\ttraining's my score: 0.0425011\tvalid_1's binary_logloss: 0.238067\tvalid_1's my score: 0.0305368\n",
      "[600]\ttraining's binary_logloss: 0.219696\ttraining's my score: 0.0590946\tvalid_1's binary_logloss: 0.236568\tvalid_1's my score: 0.0415507\n",
      "[800]\ttraining's binary_logloss: 0.214734\ttraining's my score: 0.0714747\tvalid_1's binary_logloss: 0.235997\tvalid_1's my score: 0.0439761\n",
      "[1000]\ttraining's binary_logloss: 0.21016\ttraining's my score: 0.0803675\tvalid_1's binary_logloss: 0.235837\tvalid_1's my score: 0.0459245\n",
      "[1200]\ttraining's binary_logloss: 0.20599\ttraining's my score: 0.089108\tvalid_1's binary_logloss: 0.235632\tvalid_1's my score: 0.0483897\n",
      "[1400]\ttraining's binary_logloss: 0.201939\ttraining's my score: 0.0969969\tvalid_1's binary_logloss: 0.235572\tvalid_1's my score: 0.050497\n",
      "[1600]\ttraining's binary_logloss: 0.198232\ttraining's my score: 0.104523\tvalid_1's binary_logloss: 0.235406\tvalid_1's my score: 0.0524056\n",
      "[1800]\ttraining's binary_logloss: 0.194465\ttraining's my score: 0.113048\tvalid_1's binary_logloss: 0.235362\tvalid_1's my score: 0.0544334\n",
      "[2000]\ttraining's binary_logloss: 0.190941\ttraining's my score: 0.121407\tvalid_1's binary_logloss: 0.235372\tvalid_1's my score: 0.0550298\n",
      "Early stopping, best iteration is:\n",
      "[1853]\ttraining's binary_logloss: 0.193544\ttraining's my score: 0.115137\tvalid_1's binary_logloss: 0.235366\tvalid_1's my score: 0.0552684\n",
      "Fold  9 AUC : 0.797228\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.235586\ttraining's my score: 0.0128196\tvalid_1's binary_logloss: 0.240838\tvalid_1's my score: 0.00611496\n",
      "[400]\ttraining's binary_logloss: 0.226061\ttraining's my score: 0.0434516\tvalid_1's binary_logloss: 0.236567\tvalid_1's my score: 0.0321239\n",
      "[600]\ttraining's binary_logloss: 0.21995\ttraining's my score: 0.0602315\tvalid_1's binary_logloss: 0.235229\tvalid_1's my score: 0.0441908\n",
      "[800]\ttraining's binary_logloss: 0.214993\ttraining's my score: 0.0700742\tvalid_1's binary_logloss: 0.234624\tvalid_1's my score: 0.0503873\n",
      "[1000]\ttraining's binary_logloss: 0.210465\ttraining's my score: 0.0796174\tvalid_1's binary_logloss: 0.234272\tvalid_1's my score: 0.0522218\n",
      "[1200]\ttraining's binary_logloss: 0.206246\ttraining's my score: 0.0873994\tvalid_1's binary_logloss: 0.23412\tvalid_1's my score: 0.0545455\n",
      "[1400]\ttraining's binary_logloss: 0.202265\ttraining's my score: 0.0959145\tvalid_1's binary_logloss: 0.23401\tvalid_1's my score: 0.0545455\n",
      "Early stopping, best iteration is:\n",
      "[1335]\ttraining's binary_logloss: 0.203475\ttraining's my score: 0.0933175\tvalid_1's binary_logloss: 0.23404\tvalid_1's my score: 0.0567061\n",
      "Fold 10 AUC : 0.789658\n"
     ]
    }
   ],
   "source": [
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_notnull_target[feats], \n",
    "                                                            df_notnull_target['TARGET'])):\n",
    "    train_x, train_y = df_notnull_target[feats].iloc[train_idx], df_notnull_target['TARGET'].iloc[train_idx]\n",
    "    valid_x, valid_y = df_notnull_target[feats].iloc[valid_idx], df_notnull_target['TARGET'].iloc[valid_idx]\n",
    "    \n",
    "    valid_x.to_csv(\"../my_csv_files/MY_valid_x.csv\")\n",
    "    valid_y.to_csv(\"../my_csv_files/MY_valid_y.csv\")\n",
    "    \n",
    "    # Light GBM parameters found by my hyperparameter search\n",
    "    clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "    \n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            verbose= 200, early_stopping_rounds= 200, eval_metric = my_comp_score_lgbm)\n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    \n",
    "    valid_preds = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n",
    "    valid_preds_proba = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis = 0)\n",
    "    \n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "    del train_x, train_y\n",
    "    #valid_x, valid_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf395f58-2b42-40a0-af5b-ba4c06e2ce0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92c434-ba6a-4e85-b2b7-5a0c198bacdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ceca8ce-8b79-475b-8a57-831acdd8a95e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c428e4-18a9-477d-b815-ba9a87f164fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
